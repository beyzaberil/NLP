{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8638377,"sourceType":"datasetVersion","datasetId":5173133}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\n\ndata_path = '/kaggle/input/teknofest/NLP-Teknofest24/NLPJSON/1/all.jsonl'\n\n# JSON dosyasını oku\nwith open(data_path, 'r', encoding='utf-8') as f:\n    data = [json.loads(line) for line in f]\n\n# Verileri DataFrame'e dönüştür\ndf = pd.DataFrame(data)\n\nprint(df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-08T11:52:08.385308Z","iopub.execute_input":"2024-06-08T11:52:08.385863Z","iopub.status.idle":"2024-06-08T11:52:08.446811Z","shell.execute_reply.started":"2024-06-08T11:52:08.385824Z","shell.execute_reply":"2024-06-08T11:52:08.445080Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"    id                                               text  \\\n0  284  BİLATERAL MAMOGRAFİ İNCELEMESİ:\\nHer iki memen...   \n1  285  BİLATERAL MAMOGRAFİ İNCELEMESİNDE; \\nBilateral...   \n2  286  BİLATERAL MAMOGRAFİ İNCELEMESİ\\nBilateral meme...   \n3  287  BİLATERAL MAMOGRAFİ İNCELEMESİ\\nHer iki memede...   \n4  288  BİLATERAL MAMOGRAFİ İNCELEMESİ:\\nHer iki meme ...   \n\n                                               label Comments  \n0  [[32, 47, ANAT], [48, 59, ANAT], [60, 66, OBS-...       []  \n1  [[36, 50, ANAT], [51, 65, ANAT], [66, 75, OBS-...       []  \n2  [[31, 45, ANAT], [46, 60, ANAT], [61, 71, OBS-...       []  \n3  [[31, 45, ANAT], [46, 70, ANAT], [71, 78, ANAT...       []  \n4  [[32, 44, ANAT], [45, 59, ANAT], [60, 65, OBS-...       []  \n","output_type":"stream"}]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-08T11:52:08.449731Z","iopub.execute_input":"2024-06-08T11:52:08.450211Z","iopub.status.idle":"2024-06-08T11:52:23.682411Z","shell.execute_reply.started":"2024-06-08T11:52:08.450174Z","shell.execute_reply":"2024-06-08T11:52:23.680885Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\n# 'bert-base-uncased' modelini kullanarak tokenizer oluştur\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T12:12:58.663907Z","iopub.execute_input":"2024-06-08T12:12:58.664401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = df['text'].tolist()\nlabels = df['label'].tolist()\n\n# Etiketleri işle\ndef process_labels(labels):\n    processed_labels = []\n    for label in labels:\n        temp = ['O'] * len(label[0])\n        for (start, end, label_type) in label:\n            temp[start:end] = [label_type] * (end - start)\n        processed_labels.append(temp)\n    return processed_labels\n\nprocessed_labels = process_labels(labels)\n\n# Tokenizer kullanarak verileri tokenle\nfrom transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n\nencodings = tokenizer(texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\ninput_ids = encodings['input_ids']\nattention_masks = encodings['attention_mask']\n\n# Tokenler ve etiketleri eşleştir\ndef align_labels(encodings, labels):\n    encoded_labels = []\n    for i in range(len(encodings['input_ids'])):\n        word_ids = encodings.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(labels[i][word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        encoded_labels.append(label_ids)\n    return encoded_labels\n\nencoded_labels = align_labels(encodings, processed_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T12:05:56.213586Z","iopub.execute_input":"2024-06-08T12:05:56.214042Z","iopub.status.idle":"2024-06-08T12:07:56.459479Z","shell.execute_reply.started":"2024-06-08T12:05:56.214008Z","shell.execute_reply":"2024-06-08T12:07:56.457567Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizerFast\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 'google-bert/bert-base-uncased' modelini kullanarak tokenizer oluştur\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgoogle-bert/bert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://drive.google.com/drive/folders/1-k4jE91TTKIGDERAGaZfuu8lO4LM8jMc?usp=drive_link\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2094\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2097\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2098\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2099\u001b[0m     )\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'google-bert/bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'google-bert/bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."],"ename":"OSError","evalue":"Can't load tokenizer for 'google-bert/bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'google-bert/bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.","output_type":"error"}]}]}